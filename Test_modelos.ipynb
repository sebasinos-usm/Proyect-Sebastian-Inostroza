{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07853f8e-da9f-4370-b265-2cb41e3f20d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando recursos de lenguaje (NLTK)...\n",
      "\\n--- PASO 1: Convirtiendo PDFs de 'Informes_Para_Testear' a Texto ---\n",
      "Conversión a texto completada.\n",
      "\\n--- PASO 2: Buscando y filtrando informes duplicados ---\n",
      "Se encontraron 2 informes únicos para procesar.\n",
      "\\n--- PASO 3: Anonimizando informes ---\n",
      "Anonimización completada.\n",
      "\\n--- PASO 4: Cargando modelo y clasificando ---\n",
      "  - Informe: resultadoexamen (1).txt -> Resultado: No Crítico (Certeza: 23.46%)\n",
      "  - Informe: resultadoexamen.txt -> Resultado: Crítico (Certeza: 79.56%)\n",
      "\\n✅ ¡Proceso finalizado! Resultados guardados en 'resultados_finales.csv'\n",
      "Carpeta temporal 'temp_processing' eliminada.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. PREPARACIÓN DEL ENTORNO ---\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import fitz  # PyMuPDF\n",
    "import joblib\n",
    "\n",
    "print(\"Cargando recursos de lenguaje (NLTK)...\")\n",
    "# Descargar y definir las 'stopwords'\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "# --- 2. CONFIGURACIÓN DEL SCRIPT ---\n",
    "# --- ¡IMPORTANTE! MODIFICA ESTAS VARIABLES SEGÚN TUS NECESIDADES ---\n",
    "\n",
    "# Carpeta que contiene los nuevos informes en PDF que quieres clasificar\n",
    "CARPETA_NUEVOS_PDFS = 'Informes_Para_Testear'\n",
    "\n",
    "# Nombre del archivo de tu modelo entrenado (el mejor que tengas)\n",
    "NOMBRE_MODELO_GUARDADO = 'pipeline_naive_bayes.pkl' \n",
    "\n",
    "# Nombre de la carpeta temporal que se creará para los archivos intermedios\n",
    "CARPETA_TRABAJO_TEMPORAL = 'temp_processing'\n",
    "\n",
    "# Nombre del archivo final donde se guardarán los resultados\n",
    "ARCHIVO_RESULTADOS_CSV = 'resultados_finales.csv'\n",
    "\n",
    "\n",
    "# --- 3. DEFINICIÓN DE TODAS LAS FUNCIONES NECESARIAS ---\n",
    "\n",
    "def extraer_texto_de_pdf(ruta_pdf):\n",
    "    try:\n",
    "        doc = fitz.open(ruta_pdf)\n",
    "        texto = \"\".join(page.get_text() for page in doc)\n",
    "        doc.close()\n",
    "        return texto\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def calcular_hash(ruta_archivo):\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(ruta_archivo, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except IOError:\n",
    "        return None\n",
    "\n",
    "def anonimizar_informe(texto_completo):\n",
    "    marcador_inicio = r\"Fecha Examen\"\n",
    "    marcador_fin = r\"Dr(\\(a\\))?\\.|\\\\bAtentamente\\\\b\"\n",
    "    match_inicio = re.search(marcador_inicio, texto_completo, re.IGNORECASE)\n",
    "    match_fin = re.search(marcador_fin, texto_completo, re.IGNORECASE)\n",
    "    if match_inicio and match_fin:\n",
    "        cuerpo = texto_completo[match_inicio.start():match_fin.start()]\n",
    "        encabezado_anon = \"[BLOQUE PACIENTE ANONIMIZADO]\\\\n\"\n",
    "        pie_pagina_anon = \"\\\\n[BLOQUE MÉDICO ANONIMIZADO]\"\n",
    "        cuerpo_anon = re.sub(r'\\\\b\\\\d{1,2}\\\\.?\\\\d{3}\\\\.?\\\\d{3}-[\\\\dkK]\\\\b', '[ID_ANONIMIZADO]', cuerpo)\n",
    "        return encabezado_anon + cuerpo_anon + pie_pagina_anon\n",
    "    return texto_completo\n",
    "\n",
    "def remover_acentos(texto):\n",
    "    forma_nfd = unicodedata.normalize('NFD', texto)\n",
    "    return \"\".join([c for c in forma_nfd if not unicodedata.combining(c)])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = remover_acentos(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# --- 4. FUNCIÓN PRINCIPAL QUE ORQUESTA TODO EL PROCESO ---\n",
    "\n",
    "def clasificar_lote_pdf(config):\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todo el pipeline de clasificación.\n",
    "    \"\"\"\n",
    "    # Crear carpetas temporales para organizar el trabajo\n",
    "    path_texto_crudo = os.path.join(config['temp_folder'], '1_texto_crudo')\n",
    "    path_unicos = os.path.join(config['temp_folder'], '2_unicos')\n",
    "    path_anonimizados = os.path.join(config['temp_folder'], '3_anonimizados')\n",
    "    os.makedirs(path_texto_crudo, exist_ok=True)\n",
    "    os.makedirs(path_unicos, exist_ok=True)\n",
    "    os.makedirs(path_anonimizados, exist_ok=True)\n",
    "\n",
    "    # --- PASO 1: PDF a Texto ---\n",
    "    print(f\"\\\\n--- PASO 1: Convirtiendo PDFs de '{config['input_folder']}' a Texto ---\")\n",
    "    for fname in os.listdir(config['input_folder']):\n",
    "        if fname.lower().endswith(\".pdf\"):\n",
    "            ruta_pdf = os.path.join(config['input_folder'], fname)\n",
    "            texto = extraer_texto_de_pdf(ruta_pdf)\n",
    "            if texto:\n",
    "                ruta_txt = os.path.join(path_texto_crudo, os.path.splitext(fname)[0] + '.txt')\n",
    "                with open(ruta_txt, 'w', encoding='utf-8') as f:\n",
    "                    f.write(texto)\n",
    "    print(\"Conversión a texto completada.\")\n",
    "\n",
    "    # --- PASO 2: Eliminar Duplicados ---\n",
    "    print(f\"\\\\n--- PASO 2: Buscando y filtrando informes duplicados ---\")\n",
    "    hashes = {}\n",
    "    for fname in os.listdir(path_texto_crudo):\n",
    "        ruta_completa = os.path.join(path_texto_crudo, fname)\n",
    "        file_hash = calcular_hash(ruta_completa)\n",
    "        if file_hash not in hashes:\n",
    "            hashes[file_hash] = ruta_completa\n",
    "    \n",
    "    for ruta_original in hashes.values():\n",
    "        shutil.copy2(ruta_original, path_unicos)\n",
    "    print(f\"Se encontraron {len(hashes)} informes únicos para procesar.\")\n",
    "\n",
    "    # --- PASO 3: Anonimizar Textos Únicos ---\n",
    "    print(f\"\\\\n--- PASO 3: Anonimizando informes ---\")\n",
    "    for fname in os.listdir(path_unicos):\n",
    "        ruta_txt_unico = os.path.join(path_unicos, fname)\n",
    "        with open(ruta_txt_unico, 'r', encoding='utf-8') as f:\n",
    "            texto_original = f.read()\n",
    "        \n",
    "        texto_anon = anonimizar_informe(texto_original)\n",
    "        \n",
    "        ruta_txt_anon = os.path.join(path_anonimizados, fname)\n",
    "        with open(ruta_txt_anon, 'w', encoding='utf-8') as f:\n",
    "            f.write(texto_anon)\n",
    "    print(\"Anonimización completada.\")\n",
    "\n",
    "    # --- PASO 4: Cargar Modelo y Clasificar ---\n",
    "    print(f\"\\\\n--- PASO 4: Cargando modelo y clasificando ---\")\n",
    "    try:\n",
    "        pipeline_modelo = joblib.load(config['model_path'])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"¡ERROR! No se encontró el archivo del modelo: {config['model_path']}\")\n",
    "        return\n",
    "\n",
    "    resultados = []\n",
    "    for fname in os.listdir(path_anonimizados):\n",
    "        ruta_txt_final = os.path.join(path_anonimizados, fname)\n",
    "        with open(ruta_txt_final, 'r', encoding='utf-8') as f:\n",
    "            texto_a_clasificar = f.read()\n",
    "            \n",
    "        texto_limpio = preprocess_text(texto_a_clasificar)\n",
    "        \n",
    "        # Predicción\n",
    "        prediccion = pipeline_modelo.predict([texto_limpio])[0]\n",
    "        probabilidades = pipeline_modelo.predict_proba([texto_limpio])[0]\n",
    "        \n",
    "        etiqueta = \"Crítico\" if prediccion == 1 else \"No Crítico\"\n",
    "        prob_critico = probabilidades[1]\n",
    "        \n",
    "        print(f\"  - Informe: {fname} -> Resultado: {etiqueta} (Certeza: {prob_critico:.2%})\")\n",
    "        resultados.append({'Archivo': fname, 'Clasificacion': etiqueta, 'Probabilidad_Critico': prob_critico})\n",
    "\n",
    "    # --- PASO 5: Guardar Resultados ---\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados.to_csv(config['output_csv'], index=False)\n",
    "    print(f\"\\\\n✅ ¡Proceso finalizado! Resultados guardados en '{config['output_csv']}'\")\n",
    "\n",
    "    # --- PASO 6: Limpieza ---\n",
    "    shutil.rmtree(config['temp_folder'])\n",
    "    print(f\"Carpeta temporal '{config['temp_folder']}' eliminada.\")\n",
    "\n",
    "\n",
    "# --- 5. EJECUCIÓN DEL SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        'input_folder': CARPETA_NUEVOS_PDFS,\n",
    "        'model_path': NOMBRE_MODELO_GUARDADO,\n",
    "        'temp_folder': CARPETA_TRABAJO_TEMPORAL,\n",
    "        'output_csv': ARCHIVO_RESULTADOS_CSV\n",
    "    }\n",
    "    clasificar_lote_pdf(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1479b6ce-5949-4e59-8b33-2539dbb2adee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
